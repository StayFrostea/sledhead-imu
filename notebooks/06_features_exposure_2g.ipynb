{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06: Extract Random Forest Features\n",
        "\n",
        "Extract comprehensive features from IMU data for Random Forest classification.\n",
        "\n",
        "**Pipeline flow:** 01 Ingest → 06 Features → 07 Labels → 08 Model-Ready\n",
        "\n",
        "Features are extracted from normalized time-series data per run, then aggregated per athlete per day.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append('../../src')\n",
        "\n",
        "from sledhead_imu.features.random_forest_features import (\n",
        "    extract_all_runs, \n",
        "    aggregate_rf_features_daily\n",
        ")\n",
        "\n",
        "print(\"Extracting Random Forest features from IMU data...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load normalized data from previous step\n",
        "data_dir = Path('../data')\n",
        "ingest_dir = data_dir / '01_ingest_normalize'\n",
        "\n",
        "# Load all normalized IMU files\n",
        "normalized_files = list(ingest_dir.glob('normalized_sample_imu_A*.csv'))\n",
        "print(f\"Found {len(normalized_files)} normalized files\")\n",
        "\n",
        "# Load and concatenate all files\n",
        "df_all = []\n",
        "for imu_file in normalized_files:\n",
        "    df = pd.read_csv(imu_file)\n",
        "    df_all.append(df)\n",
        "\n",
        "df_raw = pd.concat(df_all, ignore_index=True)\n",
        "print(f\"Total data shape: {df_raw.shape}\")\n",
        "print(f\"Columns: {list(df_raw.columns)}\")\n",
        "print(f\"\\nUnique athletes: {df_raw['athlete_id'].unique()}\")\n",
        "print(f\"Unique runs: {df_raw['run_id'].unique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features for all runs\n",
        "features_df = extract_all_runs(df_raw, fs=2000.0)\n",
        "\n",
        "print(f\"Extracted features for {len(features_df)} runs\")\n",
        "print(f\"Feature columns: {len(features_df.columns)}\")\n",
        "print(f\"\\nFeatures DataFrame:\")\n",
        "print(features_df.head())\n",
        "\n",
        "print(f\"\\n\\nFeature columns:\")\n",
        "print(list(features_df.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate run-level features to daily per-athlete\n",
        "print(\"\\nAggregating features per athlete per day...\")\n",
        "\n",
        "# Add timestamps to features_df for aggregation\n",
        "for idx, row in features_df.iterrows():\n",
        "    run_data = df_raw[(df_raw['athlete_id'] == row['athlete_id']) & \n",
        "                      (df_raw['run_id'] == row['run_id'])]\n",
        "    if not run_data.empty:\n",
        "        features_df.loc[idx, 'timestamp'] = run_data['timestamp'].iloc[0]\n",
        "\n",
        "features_df['timestamp'] = pd.to_datetime(features_df['timestamp'])\n",
        "features_df['date'] = features_df['timestamp'].dt.date\n",
        "\n",
        "daily_features_df = aggregate_rf_features_daily(features_df)\n",
        "\n",
        "print(f\"Aggregated to {len(daily_features_df)} athlete-days\")\n",
        "print(f\"\\nDaily features:\")\n",
        "print(daily_features_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"Summary statistics of daily features:\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "print(daily_features_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save features to CSV\n",
        "# Use the standard pipeline directory structure\n",
        "output_dir = data_dir / '06_features_exposure_2g' / 'exposure_data'\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save daily aggregated features (matches pipeline structure)\n",
        "output_file = output_dir / 'rf_features_daily.csv'\n",
        "daily_features_df.to_csv(output_file, index=False)\n",
        "print(f\"\\nSaved daily features to: {output_file}\")\n",
        "\n",
        "# Also save run-level features for reference\n",
        "run_file = output_dir / 'rf_features_runs.csv'\n",
        "features_df.to_csv(run_file, index=False)\n",
        "print(f\"Saved run-level features to: {run_file}\")\n",
        "print(\"\\nDaily features are ready for label merging in the next step.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
