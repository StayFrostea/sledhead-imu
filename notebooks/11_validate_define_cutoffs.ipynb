{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11: Validate & Define Bench Cutoffs\n",
        "\n",
        "Validate Random Forest model performance and define severity-based alert thresholds.\n",
        "\n",
        "## Purpose\n",
        "\n",
        "1. **Validate** model performance on test set\n",
        "2. **Analyze** per-class metrics (severity 0-5)\n",
        "3. **Define** alert thresholds for bench decisions\n",
        "4. **Save** thresholds for production use\n",
        "\n",
        "## Alert Levels\n",
        "\n",
        "- **Severity 0-1**: No action (green)\n",
        "- **Severity 2**: Low alert - Monitor (yellow)\n",
        "- **Severity 3**: Medium alert - Caution (orange)\n",
        "- **Severity 4**: High alert - Bench recommendation (red)\n",
        "- **Severity 5**: Critical alert - Immediate intervention (dark red)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append('../../src')\n",
        "\n",
        "from sledhead_imu.models.random_forest import evaluate_random_forest\n",
        "from sledhead_imu.validate.validate_cutoffs import (\n",
        "    validate_model_performance,\n",
        "    validate_per_class_performance,\n",
        "    get_confusion_matrix_report\n",
        ")\n",
        "from sledhead_imu.alerts.thresholds import define_alert_thresholds_from_severity\n",
        "\n",
        "# Load test set and trained model\n",
        "data_dir = Path('../data')\n",
        "models_dir = data_dir / '10_models'\n",
        "validate_dir = data_dir / '11_metrics_validate_cutoffs'\n",
        "\n",
        "# Load test data\n",
        "test_X_file = data_dir / '09_splits' / 'test' / 'X_test.csv'\n",
        "test_y_file = data_dir / '09_splits' / 'test' / 'y_test.csv'\n",
        "\n",
        "if test_X_file.exists() and test_y_file.exists():\n",
        "    X_test = pd.read_csv(test_X_file)\n",
        "    y_test = pd.read_csv(test_y_file)\n",
        "    if isinstance(y_test, pd.DataFrame):\n",
        "        y_test = y_test.iloc[:, 0]\n",
        "    \n",
        "    print(\"✓ Loaded test set\")\n",
        "    print(f\"  Samples: {len(X_test)}\")\n",
        "    \n",
        "    # Load trained model\n",
        "    model_file = models_dir / 'rf' / 'model.pkl'\n",
        "    if model_file.exists():\n",
        "        with open(model_file, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(\"✓ Loaded trained Random Forest model\")\n",
        "    else:\n",
        "        print(\"⚠️  Model not found. Train model in 10_train_random_forest.ipynb first.\")\n",
        "        model = None\n",
        "else:\n",
        "    print(\"⚠️  Test set not found. Run 09_train_test_split.ipynb first.\")\n",
        "    model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model on test set\n",
        "if model is not None:\n",
        "    print(\"Evaluating model on test set...\")\n",
        "    \n",
        "    results = evaluate_random_forest(model, X_test, y_test)\n",
        "    \n",
        "    print(f\"\\n✓ Overall Performance:\")\n",
        "    print(f\"  Accuracy: {results['accuracy']:.3f}\")\n",
        "    \n",
        "    # Overall metrics\n",
        "    metrics = validate_model_performance(y_test, results['predictions'])\n",
        "    print(f\"\\n✓ Weighted Metrics:\")\n",
        "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
        "    print(f\"  Recall: {metrics['recall']:.3f}\")\n",
        "    print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
        "    \n",
        "    # Per-class performance\n",
        "    per_class = validate_per_class_performance(y_test, results['predictions'])\n",
        "    print(f\"\\n✓ Per-Class Performance:\")\n",
        "    print(per_class.to_string(index=False))\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm, report_df = get_confusion_matrix_report(y_test, results['predictions'])\n",
        "    print(f\"\\n✓ Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    \n",
        "    # Save performance report\n",
        "    validate_dir.mkdir(parents=True, exist_ok=True)\n",
        "    reports_dir = validate_dir / 'reports'\n",
        "    reports_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    per_class.to_csv(reports_dir / 'per_class_performance.csv', index=False)\n",
        "    report_df.to_csv(reports_dir / 'classification_report.csv')\n",
        "    np.savetxt(reports_dir / 'confusion_matrix.csv', cm, delimiter=',', fmt='%d')\n",
        "    \n",
        "    print(f\"\\n✓ Saved reports to {reports_dir}\")\n",
        "else:\n",
        "    print(\"⚠️  Skipping evaluation - no model available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define alert thresholds\n",
        "if model is not None:\n",
        "    print(\"Defining alert thresholds...\")\n",
        "    \n",
        "    # Define severity to alert mapping\n",
        "    severity_mapping = define_alert_thresholds_from_severity()\n",
        "    \n",
        "    print(f\"\\n✓ Alert Thresholds:\")\n",
        "    for severity, level in sorted(severity_mapping.items()):\n",
        "        print(f\"  Severity {severity}: {level.capitalize()}\")\n",
        "    \n",
        "    # Save thresholds\n",
        "    thresholds_dir = validate_dir / 'thresholds'\n",
        "    thresholds_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Save as JSON\n",
        "    with open(thresholds_dir / 'severity_mapping.json', 'w') as f:\n",
        "        json.dump(severity_mapping, f, indent=2)\n",
        "    \n",
        "    # Save as CSV for easy viewing\n",
        "    threshold_df = pd.DataFrame([\n",
        "        {'severity': s, 'alert_level': l} \n",
        "        for s, l in severity_mapping.items()\n",
        "    ])\n",
        "    threshold_df.to_csv(thresholds_dir / 'severity_mapping.csv', index=False)\n",
        "    \n",
        "    print(f\"\\n✓ Saved thresholds to {thresholds_dir}\")\n",
        "    \n",
        "    print(\"\\n✅ Validation and threshold definition complete!\")\n",
        "else:\n",
        "    print(\"⚠️  Skipping threshold definition - no model available\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
