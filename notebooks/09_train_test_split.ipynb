{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09: Train/Test Split\n",
        "\n",
        "Split model-ready data into training, validation, and test sets.\n",
        "\n",
        "## Strategy\n",
        "\n",
        "For IMU data with time series characteristics:\n",
        "- **Stratified split**: Preserves class distribution across splits\n",
        "- **Optional athlete-based split**: Ensures unseen athletes in test set\n",
        "- **Optional time-based split**: Ensures temporal separation\n",
        "\n",
        "Standard split: **60% train, 20% val, 20% test**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "sys.path.append('../../src')\n",
        "\n",
        "from sledhead_imu.prep.split_train_test import split_train_test\n",
        "\n",
        "# Load model-ready data\n",
        "data_dir = Path('../data')\n",
        "model_ready_dir = data_dir / '08_model_ready_build' / 'model_ready_data'\n",
        "splits_dir = data_dir / '09_splits'\n",
        "\n",
        "# Find model-ready files\n",
        "feature_files = list(model_ready_dir.glob('features_*.csv'))\n",
        "print(f\"Found {len(feature_files)} feature files\")\n",
        "\n",
        "if not feature_files:\n",
        "    print(\"No feature files found. Run 08_model_ready_build.ipynb first.\")\n",
        "else:\n",
        "    # Load features and labels\n",
        "    feature_file = model_ready_dir / 'features_rf.csv'  # Latest RF features\n",
        "    label_file = model_ready_dir / 'labels_rf.csv'\n",
        "    \n",
        "    if feature_file.exists() and label_file.exists():\n",
        "        X = pd.read_csv(feature_file)\n",
        "        y = pd.read_csv(label_file)\n",
        "        \n",
        "        print(f\"\\nLoaded data:\")\n",
        "        print(f\"  Samples: {len(X)}\")\n",
        "        print(f\"  Features: {X.shape[1]}\")\n",
        "        print(f\"  Label distribution:\\n{y['severity'].value_counts().sort_index()}\")\n",
        "        \n",
        "        # Check if we have enough data for splits\n",
        "        if len(X) < 3:\n",
        "            print(f\"\\n⚠️  WARNING: Only {len(X)} samples available.\")\n",
        "            print(\"Not enough data for train/val/test split.\")\n",
        "            print(\"Need at least 3 samples (ideally 10+)\")\n",
        "        else:\n",
        "            print(f\"\\n✓ Sufficient data for split\")\n",
        "    else:\n",
        "        print(f\"\\nFiles not found:\")\n",
        "        print(f\"  Features: {feature_file.exists()}\")\n",
        "        print(f\"  Labels: {label_file.exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform train/val/test split\n",
        "# Only execute if we have sufficient data\n",
        "\n",
        "if 'X' in locals() and 'y' in locals() and len(X) >= 3:\n",
        "    # First split: train+val vs test (80/20)\n",
        "    X_train_val, X_test, y_train_val, y_test = split_train_test(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Second split: train vs val (75/25 of remaining 80%)\n",
        "    X_train, X_val, y_train, y_val = split_train_test(\n",
        "        X_train_val, y_train_val, test_size=0.25, random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✓ Splits created:\")\n",
        "    print(f\"  Train: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "    print(f\"  Val: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "    print(f\"  Test: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "    \n",
        "    # Save splits\n",
        "    splits_dir.mkdir(parents=True, exist_ok=True)\n",
        "    train_dir = splits_dir / 'train'\n",
        "    val_dir = splits_dir / 'val'\n",
        "    test_dir = splits_dir / 'test'\n",
        "    \n",
        "    train_dir.mkdir(exist_ok=True)\n",
        "    val_dir.mkdir(exist_ok=True)\n",
        "    test_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save train\n",
        "    X_train.to_csv(train_dir / 'X_train.csv', index=False)\n",
        "    y_train.to_csv(train_dir / 'y_train.csv', index=False)\n",
        "    print(f\"\\n✓ Saved train split to {train_dir}\")\n",
        "    \n",
        "    # Save val\n",
        "    X_val.to_csv(val_dir / 'X_val.csv', index=False)\n",
        "    y_val.to_csv(val_dir / 'y_val.csv', index=False)\n",
        "    print(f\"✓ Saved val split to {val_dir}\")\n",
        "    \n",
        "    # Save test\n",
        "    X_test.to_csv(test_dir / 'X_test.csv', index=False)\n",
        "    y_test.to_csv(test_dir / 'y_test.csv', index=False)\n",
        "    print(f\"✓ Saved test split to {test_dir}\")\n",
        "    \n",
        "    # Show label distribution per split\n",
        "    print(f\"\\nLabel distribution by split:\")\n",
        "    print(f\"\\n  Train:\\n{y_train['severity'].value_counts().sort_index()}\")\n",
        "    print(f\"\\n  Val:\\n{y_val['severity'].value_counts().sort_index()}\")\n",
        "    print(f\"\\n  Test:\\n{y_test['severity'].value_counts().sort_index()}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n⚠️  Skipping split - insufficient data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Generate synthetic data for demonstration\n",
        "# This creates a larger dataset to show the split functionality\n",
        "\n",
        "if 'X' not in locals() or 'y' not in locals() or len(X) < 3:\n",
        "    print(\"Generating synthetic data for demonstration...\")\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    n_samples = 100\n",
        "    \n",
        "    # Create synthetic features\n",
        "    X_synth = pd.DataFrame({\n",
        "        'time_above_2.0g': np.random.exponential(10, n_samples),\n",
        "        'time_above_3.0g': np.random.exponential(3, n_samples),\n",
        "        'g_seconds_2.0g': np.random.exponential(15, n_samples),\n",
        "        'g_seconds_3.0g': np.random.exponential(5, n_samples),\n",
        "        'num_peaks_over_3g': np.random.poisson(5, n_samples),\n",
        "        'num_peaks_over_4g': np.random.poisson(2, n_samples),\n",
        "        'longest_2g_duration': np.random.exponential(0.1, n_samples),\n",
        "        'run_duration': np.random.uniform(60, 180, n_samples),\n",
        "        'accel_mean': np.random.uniform(1, 3, n_samples),\n",
        "        'accel_std': np.random.uniform(0.5, 2, n_samples),\n",
        "        'gyro_mean': np.random.uniform(0, 50, n_samples),\n",
        "        'gyro_std': np.random.uniform(0, 20, n_samples),\n",
        "        'jerk_mean': np.random.uniform(10, 100, n_samples),\n",
        "        'dominant_freq': np.random.uniform(0.01, 0.5, n_samples),\n",
        "        'accel_max': np.random.uniform(2, 10, n_samples),\n",
        "        'accel_range': np.random.uniform(1, 8, n_samples),\n",
        "        'gyro_max': np.random.uniform(20, 200, n_samples),\n",
        "        'jerk_max': np.random.uniform(100, 2000, n_samples),\n",
        "        'highest_peak_g': np.random.uniform(2, 9, n_samples),\n",
        "        'num_symptoms': np.random.randint(0, 5, n_samples),\n",
        "        'accel_min': np.random.uniform(0, 1, n_samples),\n",
        "    })\n",
        "    \n",
        "    # Create synthetic labels (severity 0-5)\n",
        "    y_synth = pd.DataFrame({\n",
        "        'severity': np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.3, 0.25, 0.2, 0.15, 0.05, 0.05])\n",
        "    })\n",
        "    \n",
        "    print(f\"✓ Created {n_samples} synthetic samples\")\n",
        "    \n",
        "    # Perform split\n",
        "    X_train_val, X_test, y_train_val, y_test = split_train_test(\n",
        "        X_synth, y_synth, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = split_train_test(\n",
        "        X_train_val, y_train_val, test_size=0.25, random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✓ Splits created from synthetic data:\")\n",
        "    print(f\"  Train: {len(X_train)} ({len(X_train)/len(X_synth)*100:.1f}%)\")\n",
        "    print(f\"  Val: {len(X_val)} ({len(X_val)/len(X_synth)*100:.1f}%)\")\n",
        "    print(f\"  Test: {len(X_test)} ({len(X_test)/len(X_synth)*100:.1f}%)\")\n",
        "    \n",
        "    # Save\n",
        "    splits_dir.mkdir(parents=True, exist_ok=True)\n",
        "    splits_dir.joinpath('train').mkdir(exist_ok=True)\n",
        "    splits_dir.joinpath('val').mkdir(exist_ok=True)\n",
        "    splits_dir.joinpath('test').mkdir(exist_ok=True)\n",
        "    \n",
        "    X_train.to_csv(splits_dir / 'train' / 'X_train.csv', index=False)\n",
        "    y_train.to_csv(splits_dir / 'train' / 'y_train.csv', index=False)\n",
        "    X_val.to_csv(splits_dir / 'val' / 'X_val.csv', index=False)\n",
        "    y_val.to_csv(splits_dir / 'val' / 'y_val.csv', index=False)\n",
        "    X_test.to_csv(splits_dir / 'test' / 'X_test.csv', index=False)\n",
        "    y_test.to_csv(splits_dir / 'test' / 'y_test.csv', index=False)\n",
        "    \n",
        "    print(f\"\\n✓ Saved splits to data/09_splits/\")\n",
        "    \n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(f\"\\n  Train:\\n{y_train['severity'].value_counts().sort_index()}\")\n",
        "    print(f\"\\n  Val:\\n{y_val['severity'].value_counts().sort_index()}\")\n",
        "    print(f\"\\n  Test:\\n{y_test['severity'].value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
